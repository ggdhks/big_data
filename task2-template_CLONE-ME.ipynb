{"cells":[{"cell_type":"markdown","source":["# task: create a user-user network from the raw `ratings` dataset\n\n* load ratings file (code provided)\n* generate a set of edges of the form `[(u,v,w)]` where each edge represents the fact that users `u`,`v` have both rated the same `w` movies (ie if they have both rated movie1, movie2, the edge weigth will be 2)\n * you should have 164054 edges\n* save the set of edges to a file as indicated below. **Make sure you name the file using your `<id>` as indicated below, to avoid conflicts with other students within the file workspace**"],"metadata":{}},{"cell_type":"code","source":["import pandas as pd\nimport pyspark.sql.functions as f\nfrom pyspark.sql.types import *\nfrom pyspark.sql import Row\nfrom operator import add\nfrom collections import Counter"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\njava.net.SocketTimeoutException\n\tat sun.nio.ch.SocketAdaptor.connect(SocketAdaptor.java:118)\n\tat org.eclipse.jetty8.client.SelectConnector.startConnection(SelectConnector.java:82)\n\tat org.eclipse.jetty8.client.HttpDestination.startNewConnection(HttpDestination.java:292)\n\tat org.eclipse.jetty8.client.HttpDestination.doSend(HttpDestination.java:597)\n\tat org.eclipse.jetty8.client.HttpDestination.send(HttpDestination.java:532)\n\tat org.eclipse.jetty8.client.HttpClient.send(HttpClient.java:180)\n\tat com.databricks.rpc.JettyClient.$anonfun$sendRawExchange$2(JettyClient.scala:244)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:235)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:230)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:227)\n\tat com.databricks.rpc.JettyClient.withAttributionContext(JettyClient.scala:32)\n\tat com.databricks.rpc.JettyClient.sendRawExchange(JettyClient.scala:234)\n\tat com.databricks.rpc.JettyClient.sendExchange(JettyClient.scala:170)\n\tat com.databricks.rpc.JettyClient.sendAsync(JettyClient.scala:108)\n\tat com.databricks.rpc.BoundRPCClient.sendAsync(BoundRPCClient.scala:47)\n\tat com.databricks.backend.daemon.driver.DriverClient.com$databricks$backend$daemon$driver$DriverClient$$send(DriverClient.scala:95)\n\tat com.databricks.backend.daemon.driver.DriverClient.$anonfun$poll$1(DriverClient.scala:283)\n\tat com.databricks.util.FutureUtils$.retry(FutureUtils.scala:89)\n\tat com.databricks.util.FutureUtils$$anonfun$retry$2.applyOrElse(FutureUtils.scala:91)\n\tat com.databricks.util.FutureUtils$$anonfun$retry$2.applyOrElse(FutureUtils.scala:89)\n\tat scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417)\n\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\tat com.databricks.threading.NamedExecutor$$anon$2.$anonfun$run$1(NamedExecutor.scala:299)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:235)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:230)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:227)\n\tat com.databricks.threading.NamedExecutor.withAttributionContext(NamedExecutor.scala:245)\n\tat com.databricks.threading.NamedExecutor$$anon$2.run(NamedExecutor.scala:299)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)"]}}],"execution_count":2},{"cell_type":"code","source":["# 1. Load ratings file (code provided)\n# IN\nRATINGS_SMALL_PARQUET = \"/FileStore/tables/ratings-small.parquet\"\n\n# OUT\nEDGES_SMALL_PARQUET = \"/FileStore/tables/180571930/edges-small.parquet\""],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["ratings = spark.read.parquet(RATINGS_SMALL_PARQUET)\ndisplay(ratings)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["ratings.count()"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["md ### count number of distinct users  -- this is the number of unique userId that you will have in your adjacency list:"],"metadata":{}},{"cell_type":"code","source":["ratings.agg(f.countDistinct('userId')).show()"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["ratings.agg(f.countDistinct('movieId')).show()"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["#add your code here. \n\nyou need to construct a RDD consisting of edges of the form:\n\n`    [(source_node, target_node, weight)]`\n    \n    for example: `[(1,6,1), (1,8,1),(2,3,1), (2,4,1)]`\n    \nplease name the RDD with edges `weightedEdges`"],"metadata":{}},{"cell_type":"code","source":["# 2. Generate a set of edges of the form [(u,v,w)]"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["# 2.1 Drop two attributes - rating, timestamp\nratings = ratings.drop('rating','timestamp')\nratings.take(10)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["# 2.2 Convert dataframe to rdd\nr = ratings.rdd\n# type(r)\nr.take(10)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["\n# 2.3 Swap - (movieId, userId)\nmovie_user = r.map(lambda x: (x[1], x[0]))\nmovie_user.collect()"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["# 2.4 Group by key: movie, {user1, user5, ...}\noutput = movie_user.groupByKey().mapValues(list)\n# output.count() # 9724\noutput.collect()"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["# 2.5 Combinate two users with the same movieId\nimport itertools\n\ndef combinations(row):\n  k = row[0]\n  return [(k, v) for v in itertools.combinations(row[1], 2)]\n\nb = output.map(combinations).flatMap(lambda x: x)\n# [(1, (1, 2)), (1, (1, 3)), (1, (1, 4)), ...]\n\n# 2.6 Caluculate the count\nc = b.map(lambda x: x[1]).countByValue()\n# [(1, 2), (1, 4), (2, 4), (3, 4), (3, 5), (4, 5), (1, 2), (1, 3), (2, 3)]\n# defaultdict(<class 'int'>, {(1, 2): 2, (1, 3): 1, (2, 3): 1, (4, 5): 1, (3, 4): 1, (2, 4): 1, (1, 4): 1, (3, 5): 1})\n\n# 2.7 Split the item to [(1,2,3), ...]\nd = sc.parallelize([c]).flatMap(lambda x: x.items())\n# [((1, 2), 2), ((1, 3), 1),((2, 3), 1),...]\n\nd.count() # 164054"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["d.take(10) # display the result"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["# 2.8 Flat the list and sort by source_node\ne = d.map(lambda x: (x[0][0],x[0][1],x[1])).sortBy(lambda x: (x[0], x[1]))# [(266, 351, 6),...]"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["e.collect()"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["# 2.9 Rdd to dataframe\nweightedEdges = sqlContext.createDataFrame(e, ['source_node', 'target_node', 'weight'])\ndisplay(weightedEdges)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["# 3. Save the set of edges to a file as indicated below. "],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["weightedEdges.withColumnRenamed('count','weight').write.parquet(EDGES_SMALL_PARQUET, mode=\"overwrite\")"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["r = spark.read.parquet(EDGES_SMALL_PARQUET)\ndisplay(r)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["r.count()"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":24}],"metadata":{"name":"Preprocessing","notebookId":2796515577182023},"nbformat":4,"nbformat_minor":0}
